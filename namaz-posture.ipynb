{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9389416,"sourceType":"datasetVersion","datasetId":5697418}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-14T04:16:35.420377Z","iopub.execute_input":"2024-09-14T04:16:35.421377Z","iopub.status.idle":"2024-09-14T04:16:35.858792Z","shell.execute_reply.started":"2024-09-14T04:16:35.421319Z","shell.execute_reply":"2024-09-14T04:16:35.857649Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-09-14T04:16:45.544210Z","iopub.execute_input":"2024-09-14T04:16:45.545723Z","iopub.status.idle":"2024-09-14T04:17:03.827025Z","shell.execute_reply.started":"2024-09-14T04:16:45.545659Z","shell.execute_reply":"2024-09-14T04:17:03.825883Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.2.93-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.14.0)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.4.0)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.19.0)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.2.2)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.6-py3-none-any.whl.metadata (9.1 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nDownloading ultralytics-8.2.93-py3-none-any.whl (871 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m871.6/871.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.6-py3-none-any.whl (26 kB)\nInstalling collected packages: ultralytics-thop, ultralytics\nSuccessfully installed ultralytics-8.2.93 ultralytics-thop-2.0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install roboflow","metadata":{"execution":{"iopub.status.busy":"2024-09-14T04:17:03.829382Z","iopub.execute_input":"2024-09-14T04:17:03.829767Z","iopub.status.idle":"2024-09-14T04:17:19.749694Z","shell.execute_reply.started":"2024-09-14T04:17:03.829730Z","shell.execute_reply":"2024-09-14T04:17:19.748541Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting roboflow\n  Downloading roboflow-1.1.44-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from roboflow) (2024.7.4)\nRequirement already satisfied: idna==3.7 in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.7)\nRequirement already satisfied: cycler in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.4.5)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.7.5)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.26.4)\nRequirement already satisfied: opencv-python-headless==4.10.0.84 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.10.0.84)\nRequirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from roboflow) (9.5.0)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.9.0.post0)\nRequirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.32.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.16.0)\nRequirement already satisfied: urllib3>=1.26.6 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.26.18)\nRequirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.66.4)\nRequirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (6.0.2)\nRequirement already satisfied: requests-toolbelt in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.1)\nCollecting filetype (from roboflow)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (1.2.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (4.53.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->roboflow) (3.3.2)\nDownloading roboflow-1.1.44-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m79.9/79.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nInstalling collected packages: filetype, roboflow\nSuccessfully installed filetype-1.2.0 roboflow-1.1.44\n","output_type":"stream"}]},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"1BbLUMOGpdntJENr6Cy2\")\nproject = rf.workspace(\"datasalat\").project(\"salat-posture-recognition-dataset-by-belkis-hassani\")\nversion = project.version(2)\ndataset = version.download(\"yolov8\")\n                ","metadata":{"execution":{"iopub.status.busy":"2024-09-14T04:17:19.751534Z","iopub.execute_input":"2024-09-14T04:17:19.752676Z","iopub.status.idle":"2024-09-14T04:17:34.219006Z","shell.execute_reply.started":"2024-09-14T04:17:19.752622Z","shell.execute_reply":"2024-09-14T04:17:34.218010Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\nDependency ultralytics==8.0.196 is required but found version=8.2.93, to fix: `pip install ultralytics==8.0.196`\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2 to yolov8:: 100%|| 201303/201303 [00:05<00:00, 36507.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2 in yolov8:: 100%|| 8462/8462 [00:01<00:00, 6217.02it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"from ultralytics import YOLO","metadata":{"execution":{"iopub.status.busy":"2024-09-14T04:17:49.768066Z","iopub.execute_input":"2024-09-14T04:17:49.768977Z","iopub.status.idle":"2024-09-14T04:17:49.773760Z","shell.execute_reply.started":"2024-09-14T04:17:49.768931Z","shell.execute_reply":"2024-09-14T04:17:49.772529Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = YOLO(\"yolov8n\")","metadata":{"execution":{"iopub.status.busy":"2024-09-14T04:17:54.522526Z","iopub.execute_input":"2024-09-14T04:17:54.523122Z","iopub.status.idle":"2024-09-14T04:17:55.046808Z","shell.execute_reply.started":"2024-09-14T04:17:54.523078Z","shell.execute_reply":"2024-09-14T04:17:55.045831Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|| 6.25M/6.25M [00:00<00:00, 58.8MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Define the paths to the train, test, and valid directories\ntrain_dir = '/kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/train'\ntest_dir = '/kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/test'\nvalid_dir = '/kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/valid'\n\n# Function to count images in a directory\ndef count_images(directory):\n    return sum([len(files) for r, d, files in os.walk(directory)])\n\n# Count images in each directory\ntrain_images = count_images(train_dir)\ntest_images = count_images(test_dir)\nvalid_images = count_images(valid_dir)\n\n# Print the results\nprint(f\"Total images in Train set: {train_images}\")\nprint(f\"Total images in Test set: {test_images}\")\nprint(f\"Total images in Valid set: {valid_images}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T04:17:59.136593Z","iopub.execute_input":"2024-09-14T04:17:59.137033Z","iopub.status.idle":"2024-09-14T04:17:59.163292Z","shell.execute_reply.started":"2024-09-14T04:17:59.136992Z","shell.execute_reply":"2024-09-14T04:17:59.162249Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Total images in Train set: 7384\nTotal images in Test set: 354\nTotal images in Valid set: 712\n","output_type":"stream"}]},{"cell_type":"code","source":"import yaml\n\n\n# Load the YAML file\nyaml_file_path = '/kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/data.yaml'  # Example YAML file path\n\n# Define the paths to your dataset directories\ntrain_dir = '/kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/train'\ntest_dir = '/kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/test'\nvalid_dir = '/kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/valid'\n\n# Read the YAML file\nwith open(yaml_file_path, 'r') as file:\n    yaml_content = yaml.safe_load(file)\n\n# Update the paths in the YAML file (ensure the keys match your YAML file's structure)\nyaml_content['train'] = train_dir\nyaml_content['test'] = test_dir\nyaml_content['val'] = valid_dir  # If you have validation images\n\n# Write the updated paths back to the YAML file\nwith open(yaml_file_path, 'w') as file:\n    yaml.dump(yaml_content, file)\n\nprint(f\"Updated YAML file paths for train, test, and valid images.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T04:18:06.841616Z","iopub.execute_input":"2024-09-14T04:18:06.842490Z","iopub.status.idle":"2024-09-14T04:18:06.856054Z","shell.execute_reply.started":"2024-09-14T04:18:06.842434Z","shell.execute_reply":"2024-09-14T04:18:06.854974Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Updated YAML file paths for train, test, and valid images.\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(12345)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T04:18:16.425350Z","iopub.execute_input":"2024-09-14T04:18:16.426337Z","iopub.status.idle":"2024-09-14T04:18:16.431045Z","shell.execute_reply.started":"2024-09-14T04:18:16.426288Z","shell.execute_reply":"2024-09-14T04:18:16.429910Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"results = model.train(\n    data=\"/kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/data.yaml\",\n    epochs=60,\n    project='Namaz posture detection',  # Updated project name\n    name='Namaz_posture_detection_project',\n    save_period=3\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T04:18:34.678807Z","iopub.execute_input":"2024-09-14T04:18:34.679732Z","iopub.status.idle":"2024-09-14T05:09:09.753654Z","shell.execute_reply.started":"2024-09-14T04:18:34.679683Z","shell.execute_reply":"2024-09-14T05:09:09.752633Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.93  Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/data.yaml, epochs=60, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=3, cache=False, device=None, workers=8, project=Namaz posture detection, name=Namaz_posture_detection_project, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=Namaz posture detection/Namaz_posture_detection_project\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|| 755k/755k [00:00<00:00, 14.2MB/s]\n2024-09-14 04:18:36,393\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n2024-09-14 04:18:37,019\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=7\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    752677  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \nModel summary: 225 layers, 3,012,213 parameters, 3,012,197 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir Namaz posture detection/Namaz_posture_detection_project', view at http://localhost:6006/\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240914_041929-wlhepwov</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/farhanfiaz79-self/Namaz%20posture%20detection/runs/wlhepwov' target=\"_blank\">Namaz_posture_detection_project</a></strong> to <a href='https://wandb.ai/farhanfiaz79-self/Namaz%20posture%20detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/farhanfiaz79-self/Namaz%20posture%20detection' target=\"_blank\">https://wandb.ai/farhanfiaz79-self/Namaz%20posture%20detection</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/farhanfiaz79-self/Namaz%20posture%20detection/runs/wlhepwov' target=\"_blank\">https://wandb.ai/farhanfiaz79-self/Namaz%20posture%20detection/runs/wlhepwov</a>"},"metadata":{}},{"name":"stdout","text":"Freezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/train/labels... 3692 images, 325 backgrounds, 0 corrupt: 100%|| 3692/3692 [00:03<00:00, 1090.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/train/labels.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/valid/labels... 356 images, 34 backgrounds, 0 corrupt: 100%|| 356/356 [00:00<00:00, 1218.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/Salat-Posture-Recognition-Dataset-by-Belkis-Hassani-2/valid/labels.cache\nPlotting labels to Namaz posture detection/Namaz_posture_detection_project/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mNamaz posture detection/Namaz_posture_detection_project\u001b[0m\nStarting training for 60 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/60      2.37G      1.285      2.693      1.593         27        640: 100%|| 231/231 [00:50<00:00,  4.58it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:03<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.523      0.486      0.436      0.264\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/60      2.33G      1.299      2.008      1.576         39        640: 100%|| 231/231 [00:46<00:00,  4.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.37it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.527      0.553      0.443       0.25\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/60      2.33G      1.326      1.812      1.582         22        640: 100%|| 231/231 [00:45<00:00,  5.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  4.85it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.443       0.32      0.275     0.0954\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/60      2.32G      1.301      1.655      1.573         25        640: 100%|| 231/231 [00:45<00:00,  5.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.34it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.416      0.483      0.466       0.27\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/60      2.32G      1.261      1.491      1.527         36        640: 100%|| 231/231 [00:45<00:00,  5.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  4.76it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.514      0.564       0.57      0.346\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/60      2.32G      1.245      1.414      1.515         31        640: 100%|| 231/231 [00:45<00:00,  5.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.551      0.556      0.561      0.319\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/60      2.32G      1.206      1.329      1.479         40        640: 100%|| 231/231 [00:45<00:00,  5.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  4.86it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.439      0.604      0.596      0.371\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/60      2.31G      1.171      1.256      1.461         50        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.37it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447       0.74      0.554      0.646      0.406\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/60      2.32G      1.177      1.224      1.464         28        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  4.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.629      0.716      0.682      0.452\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/60      2.36G      1.152      1.197       1.44         28        640: 100%|| 231/231 [00:45<00:00,  5.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.652      0.666      0.689      0.458\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      11/60      2.36G      1.147      1.149      1.433         29        640: 100%|| 231/231 [00:45<00:00,  5.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.29it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.596      0.676      0.665      0.438\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      12/60      2.31G      1.112      1.098      1.408         33        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.61it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.678      0.715      0.711      0.467\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      13/60      2.32G       1.09      1.069      1.392         37        640: 100%|| 231/231 [00:45<00:00,  5.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.10it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.608      0.698      0.677      0.442\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      14/60      2.32G      1.085      1.025       1.39         39        640: 100%|| 231/231 [00:45<00:00,  5.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.52it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.699      0.656      0.707      0.493\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      15/60      2.32G      1.067      1.011      1.375         31        640: 100%|| 231/231 [00:45<00:00,  5.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.08it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.638      0.771      0.768      0.498\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      16/60      2.31G      1.059      1.002      1.371         33        640: 100%|| 231/231 [00:44<00:00,  5.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.51it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.656      0.699       0.71      0.484\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      17/60      2.32G      1.055     0.9711      1.372         25        640: 100%|| 231/231 [00:44<00:00,  5.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.01it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.629      0.735      0.713        0.5\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      18/60      2.36G      1.033     0.9577      1.361         25        640: 100%|| 231/231 [00:45<00:00,  5.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.636      0.702        0.7      0.466\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      19/60      2.36G      1.017     0.9035      1.345         28        640: 100%|| 231/231 [00:44<00:00,  5.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  4.97it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447       0.64      0.782      0.753      0.512\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      20/60      2.31G       1.02     0.9301      1.346         32        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.55it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.715      0.736      0.735      0.504\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      21/60      2.32G     0.9992     0.8861      1.329         25        640: 100%|| 231/231 [00:45<00:00,  5.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.00it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.652      0.688      0.721      0.489\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      22/60      2.32G      1.014     0.8836      1.343         31        640: 100%|| 231/231 [00:45<00:00,  5.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.684      0.652      0.698      0.481\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      23/60      2.32G      0.979     0.8523      1.311         30        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  4.76it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.639      0.798      0.756      0.521\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      24/60      2.31G     0.9641     0.8278      1.302         29        640: 100%|| 231/231 [00:45<00:00,  5.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.727      0.779      0.757      0.523\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      25/60      2.32G     0.9708     0.8197      1.311         30        640: 100%|| 231/231 [00:44<00:00,  5.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  4.92it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.586      0.721        0.7       0.49\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      26/60      2.32G      0.971     0.8234      1.306         27        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.698      0.724      0.729       0.51\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      27/60      2.36G     0.9656     0.7932      1.303         19        640: 100%|| 231/231 [00:45<00:00,  5.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.05it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.714       0.75       0.75      0.521\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      28/60      2.31G     0.9439     0.7846      1.283         39        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.789       0.64      0.755      0.533\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      29/60      2.32G     0.9422     0.7784      1.288         27        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  4.84it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.703      0.687      0.745      0.514\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      30/60      2.32G      0.917     0.7491      1.275         21        640: 100%|| 231/231 [00:45<00:00,  5.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.61it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.707      0.759      0.737      0.523\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      31/60      2.32G     0.9311     0.7543       1.27         48        640: 100%|| 231/231 [00:45<00:00,  5.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.07it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447       0.74      0.737      0.745      0.519\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      32/60      2.31G     0.9064      0.737      1.264         26        640: 100%|| 231/231 [00:45<00:00,  5.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.685       0.75      0.744      0.541\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      33/60      2.32G     0.9101     0.7156      1.268         29        640: 100%|| 231/231 [00:45<00:00,  5.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.03it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.757      0.709      0.736       0.53\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      34/60      2.36G     0.9023      0.707      1.256         25        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.704       0.75      0.763      0.554\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      35/60      2.32G     0.8909     0.7057      1.256         30        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.09it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.722      0.812      0.786      0.568\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      36/60      2.31G     0.8832     0.6976      1.246         27        640: 100%|| 231/231 [00:44<00:00,  5.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.811      0.697      0.775      0.553\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      37/60      2.32G      0.872     0.6703      1.249         40        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  4.90it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.654      0.735      0.753      0.538\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      38/60      2.32G     0.8648     0.6583      1.229         30        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.52it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.746      0.713       0.73      0.532\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      39/60      2.32G     0.8563     0.6684      1.237         41        640: 100%|| 231/231 [00:45<00:00,  5.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.13it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.751      0.712      0.756      0.543\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      40/60      2.31G     0.8594     0.6546      1.228         26        640: 100%|| 231/231 [00:44<00:00,  5.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.707      0.759      0.774      0.553\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      41/60      2.32G     0.8419     0.6464      1.224         31        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447       0.71        0.7       0.73      0.529\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      42/60      2.32G     0.8413     0.6418      1.224         46        640: 100%|| 231/231 [00:44<00:00,  5.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.758      0.684      0.755      0.544\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      43/60      2.32G     0.8493     0.6391      1.226         28        640: 100%|| 231/231 [00:45<00:00,  5.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.758      0.707      0.754      0.529\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      44/60      2.31G     0.8354     0.6316      1.217         30        640: 100%|| 231/231 [00:44<00:00,  5.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.52it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.701      0.718      0.725      0.523\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      45/60      2.32G     0.8193     0.6183      1.205         23        640: 100%|| 231/231 [00:45<00:00,  5.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.12it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.709      0.746       0.75      0.532\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      46/60      2.32G     0.8069     0.6078      1.194         32        640: 100%|| 231/231 [00:45<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.677        0.7      0.729      0.519\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      47/60      2.32G     0.8012     0.5872        1.2         30        640: 100%|| 231/231 [00:45<00:00,  5.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.26it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.692      0.764      0.744       0.54\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      48/60      2.31G     0.8057     0.5945      1.199         34        640: 100%|| 231/231 [00:45<00:00,  5.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.718      0.729      0.753       0.53\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      49/60      2.32G     0.7817     0.5848      1.188         33        640: 100%|| 231/231 [00:45<00:00,  5.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.07it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447       0.69      0.764      0.752      0.541\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      50/60      2.32G     0.7894     0.5742      1.192         36        640: 100%|| 231/231 [00:45<00:00,  5.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.711      0.735      0.725      0.525\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Closing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      51/60      2.32G     0.6947     0.4037      1.201         14        640: 100%|| 231/231 [00:46<00:00,  5.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.02it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.699      0.802      0.773      0.545\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      52/60      2.31G     0.6837     0.3847      1.197          9        640: 100%|| 231/231 [00:43<00:00,  5.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.58it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.767      0.692      0.743       0.53\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      53/60      2.32G     0.6688     0.3705      1.184         14        640: 100%|| 231/231 [00:44<00:00,  5.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447       0.76      0.737       0.76      0.542\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      54/60      2.32G     0.6522     0.3678      1.166         20        640: 100%|| 231/231 [00:44<00:00,  5.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.755      0.767      0.775      0.555\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      55/60      2.36G     0.6416     0.3556      1.159         26        640: 100%|| 231/231 [00:44<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447       0.72      0.778      0.737      0.525\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      56/60      2.31G       0.63     0.3485      1.143         10        640: 100%|| 231/231 [00:44<00:00,  5.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.54it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.786      0.743      0.758      0.547\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      57/60      2.32G     0.6171      0.338      1.141         18        640: 100%|| 231/231 [00:43<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.73it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.766      0.743      0.761      0.547\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      58/60      2.32G     0.6146     0.3359       1.14         12        640: 100%|| 231/231 [00:44<00:00,  5.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.772      0.722      0.745       0.54\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      59/60      2.32G     0.6019     0.3313      1.131         10        640: 100%|| 231/231 [00:44<00:00,  5.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.61it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.776      0.743      0.747       0.54\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      60/60      2.31G     0.5951     0.3296      1.118         12        640: 100%|| 231/231 [00:44<00:00,  5.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:02<00:00,  5.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.766      0.753       0.75      0.543\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n60 epochs completed in 0.812 hours.\nOptimizer stripped from Namaz posture detection/Namaz_posture_detection_project/weights/last.pt, 6.3MB\nOptimizer stripped from Namaz posture detection/Namaz_posture_detection_project/weights/best.pt, 6.3MB\n\nValidating Namaz posture detection/Namaz_posture_detection_project/weights/best.pt...\nUltralytics YOLOv8.2.93  Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 168 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 12/12 [00:04<00:00,  2.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        356        447      0.725      0.808      0.785      0.568\n               raising         99        119      0.927      0.975      0.977      0.687\n                  ruku         68        152      0.627      0.697      0.745      0.469\n               sitting         35         35      0.714      0.857       0.85      0.706\n                 sujud         87         91      0.967      0.955      0.975      0.736\n               takbeer         10         10      0.612      0.789      0.669      0.455\n               tashhud         21         24       0.61      0.583       0.69      0.527\n               tasleem         12         16      0.615      0.798       0.59      0.397\nSpeed: 0.2ms preprocess, 2.0ms inference, 0.0ms loss, 3.1ms postprocess per image\nResults saved to \u001b[1mNamaz posture detection/Namaz_posture_detection_project\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='14.186 MB of 14.186 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td></td></tr><tr><td>lr/pg1</td><td></td></tr><tr><td>lr/pg2</td><td></td></tr><tr><td>metrics/mAP50(B)</td><td></td></tr><tr><td>metrics/mAP50-95(B)</td><td></td></tr><tr><td>metrics/precision(B)</td><td></td></tr><tr><td>metrics/recall(B)</td><td></td></tr><tr><td>model/GFLOPs</td><td></td></tr><tr><td>model/parameters</td><td></td></tr><tr><td>model/speed_PyTorch(ms)</td><td></td></tr><tr><td>train/box_loss</td><td></td></tr><tr><td>train/cls_loss</td><td></td></tr><tr><td>train/dfl_loss</td><td></td></tr><tr><td>val/box_loss</td><td></td></tr><tr><td>val/cls_loss</td><td></td></tr><tr><td>val/dfl_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>2e-05</td></tr><tr><td>lr/pg1</td><td>2e-05</td></tr><tr><td>lr/pg2</td><td>2e-05</td></tr><tr><td>metrics/mAP50(B)</td><td>0.78542</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.56817</td></tr><tr><td>metrics/precision(B)</td><td>0.72454</td></tr><tr><td>metrics/recall(B)</td><td>0.80768</td></tr><tr><td>model/GFLOPs</td><td>8.201</td></tr><tr><td>model/parameters</td><td>3012213</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>2.327</td></tr><tr><td>train/box_loss</td><td>0.59515</td></tr><tr><td>train/cls_loss</td><td>0.32959</td></tr><tr><td>train/dfl_loss</td><td>1.11775</td></tr><tr><td>val/box_loss</td><td>1.05827</td></tr><tr><td>val/cls_loss</td><td>0.7078</td></tr><tr><td>val/dfl_loss</td><td>1.45672</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">Namaz_posture_detection_project</strong> at: <a href='https://wandb.ai/farhanfiaz79-self/Namaz%20posture%20detection/runs/wlhepwov' target=\"_blank\">https://wandb.ai/farhanfiaz79-self/Namaz%20posture%20detection/runs/wlhepwov</a><br/> View project at: <a href='https://wandb.ai/farhanfiaz79-self/Namaz%20posture%20detection' target=\"_blank\">https://wandb.ai/farhanfiaz79-self/Namaz%20posture%20detection</a><br/>Synced 5 W&B file(s), 24 media file(s), 5 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240914_041929-wlhepwov/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"import cv2","metadata":{"execution":{"iopub.status.busy":"2024-09-14T05:16:52.394428Z","iopub.execute_input":"2024-09-14T05:16:52.394951Z","iopub.status.idle":"2024-09-14T05:16:52.400670Z","shell.execute_reply.started":"2024-09-14T05:16:52.394907Z","shell.execute_reply":"2024-09-14T05:16:52.399519Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"input_video_path = '/kaggle/input/namaz-testing-video/namaz.mp4'\noutput_video_path = '/kaggle/working/output_video_annotated.mp4'\n\n# Open the input video using OpenCV\ncap = cv2.VideoCapture(input_video_path)\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\n# Define the codec and create VideoWriter object for the output video\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'mp4v' codec for .mp4\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Perform prediction on the frame\n    results = model(frame)\n    \n    # Annotate the frame with bounding boxes and labels\n    annotated_frame = results[0].plot()  # This will draw the bounding boxes and labels\n    \n    # Write the annotated frame to the output video\n    out.write(annotated_frame)\n\n# Release video objects\ncap.release()\nout.release()\n\nprint(\"Video processing complete. Saved to\", output_video_path)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T05:17:03.032544Z","iopub.execute_input":"2024-09-14T05:17:03.033410Z","iopub.status.idle":"2024-09-14T05:17:28.057243Z","shell.execute_reply.started":"2024-09-14T05:17:03.033360Z","shell.execute_reply":"2024-09-14T05:17:28.056122Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\n0: 384x640 1 sujud, 62.6ms\nSpeed: 3.3ms preprocess, 62.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 2.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 11.4ms\nSpeed: 3.8ms preprocess, 11.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 3.4ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.3ms\nSpeed: 3.3ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 2.7ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 11.5ms\nSpeed: 3.8ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 2.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.1ms\nSpeed: 3.8ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.7ms\nSpeed: 3.8ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.3ms\nSpeed: 3.7ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 3.1ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.9ms\nSpeed: 3.4ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 9.7ms\nSpeed: 3.6ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.8ms\nSpeed: 3.7ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 3.2ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 3.7ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.7ms\nSpeed: 3.6ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 3.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 3.6ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 2.6ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 11.0ms\nSpeed: 3.6ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.9ms\nSpeed: 3.3ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 2.6ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 2.6ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.8ms\nSpeed: 3.6ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.9ms\nSpeed: 3.6ms preprocess, 8.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.6ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 2.6ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.0ms\nSpeed: 3.9ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 2.6ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 3.6ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.9ms\nSpeed: 3.6ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.7ms\nSpeed: 3.3ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 11.1ms\nSpeed: 3.9ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 3.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 11.8ms\nSpeed: 3.7ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 2.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.7ms\nSpeed: 2.4ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.8ms\nSpeed: 2.7ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 2.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 3.4ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 11.3ms\nSpeed: 3.9ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 2.6ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 3.4ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 3.4ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 11.7ms\nSpeed: 3.8ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 2.6ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.2ms\nSpeed: 2.8ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 2.5ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.4ms\nSpeed: 3.8ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.6ms\nSpeed: 3.7ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 9.6ms\nSpeed: 3.8ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.1ms\nSpeed: 3.8ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.9ms\nSpeed: 3.9ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 3.4ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 9.9ms\nSpeed: 3.8ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 8.3ms\nSpeed: 2.4ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 8.6ms\nSpeed: 3.4ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 8.4ms\nSpeed: 2.8ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 9.6ms\nSpeed: 3.7ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 11.3ms\nSpeed: 3.8ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 8.5ms\nSpeed: 2.6ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 9.5ms\nSpeed: 3.8ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 8.7ms\nSpeed: 2.5ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.1ms\nSpeed: 2.6ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 9.2ms\nSpeed: 3.5ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.3ms\nSpeed: 2.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 9.8ms\nSpeed: 3.7ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 9.5ms\nSpeed: 3.7ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.6ms\nSpeed: 3.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.6ms\nSpeed: 3.1ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.5ms\nSpeed: 3.6ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.9ms\nSpeed: 3.4ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.6ms\nSpeed: 3.4ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.6ms\nSpeed: 3.4ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.5ms\nSpeed: 2.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 9.3ms\nSpeed: 3.8ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.5ms\nSpeed: 3.4ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.2ms\nSpeed: 2.7ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.4ms\nSpeed: 2.9ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.8ms\nSpeed: 2.7ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 11.9ms\nSpeed: 3.9ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 9.1ms\nSpeed: 3.9ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 9.2ms\nSpeed: 3.8ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.3ms\nSpeed: 3.4ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 9.3ms\nSpeed: 3.8ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 10.3ms\nSpeed: 3.5ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 10.8ms\nSpeed: 3.6ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 10.0ms\nSpeed: 2.7ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 1 tashhud, 9.3ms\nSpeed: 3.8ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.3ms\nSpeed: 3.4ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.7ms\nSpeed: 3.3ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.1ms\nSpeed: 3.7ms preprocess, 10.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 3.4ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.4ms\nSpeed: 3.8ms preprocess, 11.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.4ms\nSpeed: 3.6ms preprocess, 10.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 9.4ms\nSpeed: 3.7ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sitting, 8.8ms\nSpeed: 4.0ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 10.1ms\nSpeed: 3.8ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.1ms\nSpeed: 2.4ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 11.4ms\nSpeed: 3.8ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 9.3ms\nSpeed: 3.8ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 11.1ms\nSpeed: 3.9ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 8.3ms\nSpeed: 2.7ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 9.7ms\nSpeed: 3.7ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 10.0ms\nSpeed: 3.8ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.6ms\nSpeed: 3.5ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.5ms\nSpeed: 3.5ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.7ms\nSpeed: 3.4ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.0ms\nSpeed: 2.5ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 sujud, 9.6ms\nSpeed: 3.6ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 8.7ms\nSpeed: 3.3ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 1 sujud, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 9.7ms\nSpeed: 3.7ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.9ms\nSpeed: 3.4ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 11.3ms\nSpeed: 3.7ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 9.8ms\nSpeed: 3.7ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 11.7ms\nSpeed: 3.7ms preprocess, 11.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 9.4ms\nSpeed: 3.7ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.5ms\nSpeed: 2.5ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.4ms\nSpeed: 3.5ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 ruku, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 1 ruku, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 1 ruku, 10.1ms\nSpeed: 3.5ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 1 ruku, 9.1ms\nSpeed: 3.2ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 1 ruku, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.6ms\nSpeed: 3.9ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.8ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.2ms\nSpeed: 3.8ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.6ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 12.8ms\nSpeed: 3.9ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.4ms\nSpeed: 3.8ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.6ms\nSpeed: 3.6ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.1ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.3ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.6ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.1ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.6ms\nSpeed: 3.8ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.2ms\nSpeed: 3.8ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 2.5ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.1ms\nSpeed: 3.7ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.2ms\nSpeed: 3.6ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.7ms\nSpeed: 3.9ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.9ms\nSpeed: 3.8ms preprocess, 11.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.9ms\nSpeed: 3.8ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.4ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.3ms\nSpeed: 3.9ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.2ms\nSpeed: 3.8ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.2ms\nSpeed: 3.7ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.3ms\nSpeed: 2.9ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.2ms\nSpeed: 3.5ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.9ms\nSpeed: 3.8ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.1ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.5ms\nSpeed: 3.9ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.9ms\nSpeed: 3.7ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.5ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.4ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 2.7ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.8ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.5ms\nSpeed: 3.1ms preprocess, 11.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.7ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 7.9ms\nSpeed: 3.2ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.1ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 13.5ms\nSpeed: 4.2ms preprocess, 13.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 13.4ms\nSpeed: 3.8ms preprocess, 13.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.5ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.0ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.3ms\nSpeed: 2.4ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 7.9ms\nSpeed: 3.4ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.0ms\nSpeed: 3.7ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.4ms\nSpeed: 3.8ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.4ms\nSpeed: 3.6ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.7ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.1ms\nSpeed: 3.7ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.9ms\nSpeed: 3.6ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.2ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.4ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.6ms\nSpeed: 3.9ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.4ms\nSpeed: 3.8ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.4ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.6ms\nSpeed: 3.8ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.4ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.9ms\nSpeed: 3.8ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.6ms\nSpeed: 3.9ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.6ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.2ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.8ms\nSpeed: 3.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 2.6ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.6ms\nSpeed: 3.7ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.4ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.6ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.5ms\nSpeed: 4.0ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.6ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.9ms\nSpeed: 2.7ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.2ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.3ms\nSpeed: 3.5ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.8ms\nSpeed: 3.6ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.6ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.6ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.4ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.5ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.6ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.7ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 2.8ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.3ms\nSpeed: 3.7ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.7ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.6ms\nSpeed: 3.8ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.8ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.4ms preprocess, 8.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.8ms\nSpeed: 3.9ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.2ms\nSpeed: 4.1ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.1ms\nSpeed: 3.7ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.4ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.8ms\nSpeed: 3.8ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 2.8ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.5ms\nSpeed: 3.7ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.4ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.4ms\nSpeed: 3.8ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.5ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.6ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 12.0ms\nSpeed: 3.8ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.8ms\nSpeed: 3.7ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.0ms\nSpeed: 3.8ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.8ms\nSpeed: 3.8ms preprocess, 10.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.1ms\nSpeed: 3.6ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.8ms\nSpeed: 3.7ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.7ms\nSpeed: 3.6ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.4ms\nSpeed: 3.6ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.4ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.4ms\nSpeed: 3.5ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.5ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.1ms\nSpeed: 3.8ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.7ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.6ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.7ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.0ms\nSpeed: 3.6ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.1ms\nSpeed: 3.7ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.5ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.8ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.5ms\nSpeed: 3.9ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.7ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.9ms\nSpeed: 3.9ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.4ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.5ms\nSpeed: 3.8ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.6ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.9ms\nSpeed: 3.7ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.6ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.0ms\nSpeed: 2.9ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 7.8ms\nSpeed: 2.6ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.0ms\nSpeed: 3.7ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.7ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.8ms\nSpeed: 3.7ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.8ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.9ms\nSpeed: 4.0ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.6ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.9ms\nSpeed: 3.8ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.7ms\nSpeed: 3.8ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.7ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.1ms\nSpeed: 3.9ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.3ms\nSpeed: 2.8ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.3ms\nSpeed: 3.9ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.4ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.1ms\nSpeed: 3.8ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.5ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.7ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 12.1ms\nSpeed: 3.9ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.5ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.9ms\nSpeed: 3.8ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.8ms\nSpeed: 3.9ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.6ms\nSpeed: 3.9ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.0ms\nSpeed: 3.7ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.1ms\nSpeed: 3.7ms preprocess, 9.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.0ms\nSpeed: 3.8ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.8ms\nSpeed: 3.9ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.2ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.7ms\nSpeed: 3.6ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.8ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.5ms preprocess, 8.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.8ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.8ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.6ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.7ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.5ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.6ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.5ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.5ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.3ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.5ms\nSpeed: 3.8ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.5ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.5ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.4ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.6ms\nSpeed: 3.8ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.6ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.4ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.0ms\nSpeed: 3.4ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.2ms\nSpeed: 3.7ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.0ms\nSpeed: 3.8ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.6ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.4ms\nSpeed: 3.8ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.1ms\nSpeed: 3.9ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.5ms\nSpeed: 3.8ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.0ms\nSpeed: 3.3ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.7ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.5ms\nSpeed: 3.8ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.3ms\nSpeed: 3.8ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.9ms\nSpeed: 3.6ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.6ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.9ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.6ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.4ms\nSpeed: 3.7ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.8ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.8ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.7ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.6ms\nSpeed: 3.8ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.6ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.8ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.2ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.2ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.3ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.6ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.8ms\nSpeed: 3.9ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.3ms\nSpeed: 3.8ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.6ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.7ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.6ms\nSpeed: 3.9ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.8ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.1ms\nSpeed: 3.8ms preprocess, 10.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.2ms\nSpeed: 4.0ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.5ms\nSpeed: 3.7ms preprocess, 11.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 13.0ms\nSpeed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.0ms\nSpeed: 3.2ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.5ms\nSpeed: 3.8ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.6ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.6ms\nSpeed: 3.9ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.4ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 12.3ms\nSpeed: 3.8ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.3ms\nSpeed: 3.7ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.5ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.4ms\nSpeed: 3.8ms preprocess, 11.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.9ms\nSpeed: 3.7ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 2.8ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 12.3ms\nSpeed: 3.8ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.3ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.4ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.4ms\nSpeed: 3.8ms preprocess, 11.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.2ms\nSpeed: 3.7ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.4ms\nSpeed: 3.7ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.4ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.6ms\nSpeed: 3.8ms preprocess, 10.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.5ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.7ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.8ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.5ms\nSpeed: 3.9ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.2ms\nSpeed: 3.8ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.3ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.3ms\nSpeed: 3.8ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.8ms\nSpeed: 3.9ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.5ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.2ms\nSpeed: 3.7ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.9ms\nSpeed: 2.5ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 12.3ms\nSpeed: 3.8ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.8ms\nSpeed: 2.5ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.5ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.9ms\nSpeed: 3.5ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.9ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.4ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.2ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.7ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.1ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.5ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.1ms\nSpeed: 3.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.2ms\nSpeed: 3.5ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 2.5ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.2ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.7ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.4ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.1ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 10.1ms\nSpeed: 3.7ms preprocess, 10.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.4ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.4ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.7ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.2ms\nSpeed: 3.6ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.2ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.8ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.3ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 11.2ms\nSpeed: 4.0ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.4ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.6ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 2.7ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 4.2ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.7ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.6ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.3ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.4ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.2ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.6ms\nSpeed: 3.8ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.2ms\nSpeed: 3.6ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 3.3ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.3ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.0ms\nSpeed: 3.7ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.4ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.7ms\nSpeed: 3.6ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.2ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.6ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.5ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 2.7ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.7ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.8ms\nSpeed: 3.7ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.0ms\nSpeed: 3.4ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.0ms\nSpeed: 3.4ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.3ms\nSpeed: 3.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.5ms\nSpeed: 3.9ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.5ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 2.5ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.6ms\nSpeed: 3.6ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 3.5ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.7ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 2.4ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 3.0ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.9ms\nSpeed: 3.2ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.8ms\nSpeed: 3.8ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.6ms\nSpeed: 2.5ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 9.0ms\nSpeed: 2.5ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.3ms\nSpeed: 2.5ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 raising, 8.1ms\nSpeed: 3.5ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\nVideo processing complete. Saved to /kaggle/working/output_video_annotated.mp4\n","output_type":"stream"}]},{"cell_type":"code","source":"## when we want to use this model again than simle we upload the weights file on it and load \n## it with yolo and than we can check it on any video ","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:06:08.432341Z","iopub.execute_input":"2024-09-14T06:06:08.432718Z","iopub.status.idle":"2024-09-14T06:06:08.437849Z","shell.execute_reply.started":"2024-09-14T06:06:08.432682Z","shell.execute_reply":"2024-09-14T06:06:08.436883Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# model = YOLO(\"path to our load model.pt\")","metadata":{},"execution_count":null,"outputs":[]}]}